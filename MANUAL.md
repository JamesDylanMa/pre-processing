# 사용 매뉴얼

## 목차

1. [시작하기](#시작하기)
2. [기본 사용법](#기본-사용법)
3. [고급 기능](#고급-기능)
4. [문제 해결](#문제-해결)
5. [FAQ](#faq)

## 시작하기

### 시스템 요구사항

- **운영체제**: Windows, Linux, macOS
- **Python**: 3.8 이상
- **메모리**: 최소 4GB RAM (Ollama 사용 시 8GB 이상 권장)
- **디스크 공간**: 최소 1GB (모델 다운로드 포함)

### 초기 설정

1. **Python 설치 확인**
   ```bash
   python --version
   # Python 3.8 이상이어야 합니다
   ```

2. **가상 환경 생성 (권장)**
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/Mac
   source venv/bin/activate
   ```

3. **패키지 설치**
   ```bash
   pip install -r requirements.txt
   ```

4. **Ollama 설치 (선택사항)**
   - Windows: [Ollama 공식 사이트](https://ollama.ai/download)에서 다운로드
   - Linux/Mac: `curl -fsSL https://ollama.ai/install.sh | sh`
   
   모델 다운로드:
   ```bash
   ollama pull llava
   ```

## 기본 사용법

### 1. 애플리케이션 실행

```bash
streamlit run frontend/streamlit_app.py
```

브라우저가 자동으로 열리며 `http://localhost:8501`에서 접속할 수 있습니다.

### 2. 파일 업로드

1. 웹 인터페이스의 **"파일 업로드"** 탭으로 이동
2. **"문서 파일을 업로드하세요"** 영역에 파일을 드래그 앤 드롭하거나 클릭하여 선택
3. 지원되는 파일 형식:
   - PDF (`.pdf`)
   - Word (`.doc`, `.docx`)
   - Excel (`.xls`, `.xlsx`)
   - PowerPoint (`.ppt`, `.pptx`)
   - 텍스트 (`.txt`, `.md`)
   - 이미지 (`.png`, `.jpg`, `.jpeg`)

### 3. 처리 옵션 설정

사이드바에서 다음 옵션을 설정할 수 있습니다:

- **앙상블 처리 사용**: 여러 처리기를 조합하여 사용
- **결과 비교 활성화**: 처리 결과를 비교 분석
- **Ollama AI 처리 사용**: AI 모델을 활용한 고급 처리
- **모델 선택**: Ollama 사용 시 모델 선택
- **저장 형식**: JSON 또는 Markdown 선택

### 4. 처리 시작

**"파일 업로드 및 처리 시작"** 버튼을 클릭하면 자동으로 처리됩니다.

### 5. 결과 확인

- **처리 결과 탭**: 각 처리기의 결과를 개별적으로 확인
- **비교 분석 탭**: 여러 처리기를 비교하여 최적의 방법 확인
- **다운로드 탭**: 처리된 결과 파일 다운로드

## 고급 기능

### 앙상블 처리

여러 처리기를 동시에 사용하여 더 정확한 결과를 얻을 수 있습니다.

**사용 방법:**
1. 사이드바에서 "앙상블 처리 사용" 체크
2. 파일 업로드 및 처리 시작
3. "처리 결과" 탭에서 "ensemble_processor" 결과 확인

**장점:**
- 여러 파서의 결과를 결합하여 누락된 정보 최소화
- 각 처리기의 강점을 활용

### Ollama AI 처리

Ollama를 사용하여 AI 기반 문서 분석을 수행할 수 있습니다.

**설정 방법:**
1. Ollama 설치 및 모델 다운로드 (시작하기 참조)
2. Ollama 서비스 실행 확인: `ollama list`
3. 웹 인터페이스에서 "Ollama AI 처리 사용" 체크
4. 원하는 모델 선택

**권장 모델:**
- **llava**: 이미지가 포함된 문서에 적합
- **llama3**: 텍스트 중심 문서에 적합
- **mistral**: 빠른 처리 속도가 필요한 경우

### 결과 비교 분석

여러 처리기의 결과를 비교하여 최적의 방법을 추천받을 수 있습니다.

**비교 지표:**
- 텍스트 추출 길이
- 단어 수
- 오류 발생 여부
- 테이블 추출 여부
- 메타데이터 포함 여부

**사용 방법:**
1. "결과 비교 활성화" 체크
2. 파일 처리 후 "비교 분석" 탭 확인
3. 추천 사항 및 최적 처리기 확인

## 문제 해결

### Ollama 연결 오류

**증상**: "Ollama 서비스에 연결할 수 없습니다" 메시지 표시

**해결 방법:**
1. Ollama 서비스 실행 확인
   ```bash
   ollama list
   ```
2. `config.py`에서 `OLLAMA_BASE_URL` 확인
3. 방화벽 설정 확인
4. Ollama 재시작

### 파일 업로드 실패

**증상**: 파일 업로드 시 오류 발생

**해결 방법:**
1. 파일 크기 확인 (최대 100MB)
2. 파일 형식 확인 (지원되는 형식인지)
3. 파일이 손상되지 않았는지 확인
4. 디스크 공간 확인

### 처리 속도가 느림

**원인 및 해결:**
- 큰 파일: 파일을 작은 단위로 분할하여 처리
- Ollama 사용: 더 작은 모델 사용 또는 Ollama 비활성화
- 시스템 리소스: 다른 프로그램 종료

### 패키지 설치 오류

**증상**: `pip install` 시 오류 발생

**해결 방법:**
1. pip 업그레이드
   ```bash
   pip install --upgrade pip
   ```
2. 개별 패키지 설치
   ```bash
   pip install streamlit
   pip install PyPDF2
   # 등등...
   ```
3. Python 버전 확인 (3.8 이상 필요)

## FAQ

### Q: 어떤 파일 형식을 지원하나요?

A: PDF, Word, Excel, PowerPoint, 텍스트 파일, 이미지 파일을 지원합니다. 자세한 확장자 목록은 README.md를 참조하세요.

### Q: 처리할 수 있는 최대 파일 크기는?

A: 기본적으로 100MB까지 지원합니다. `config.py`의 `MAX_FILE_SIZE`를 수정하여 변경할 수 있습니다.

### Q: Ollama 없이도 사용할 수 있나요?

A: 네, Ollama는 선택사항입니다. 기본 파서만으로도 문서 처리가 가능합니다.

### Q: 결과 파일은 어디에 저장되나요?

A: `outputs/` 디렉토리에 저장됩니다. 파일명 형식: `{file_id}_{processor_name}_{timestamp}.{format}`

### Q: 여러 파일을 동시에 처리할 수 있나요?

A: 현재 버전에서는 한 번에 하나의 파일만 처리할 수 있습니다. 여러 파일을 처리하려면 각각 업로드해야 합니다.

### Q: 처리 결과를 영구적으로 보관할 수 있나요?

A: 네, `outputs/` 디렉토리의 파일은 삭제하지 않는 한 보관됩니다. 필요시 백업을 권장합니다.

### Q: 다른 서버로 배포하려면?

A: README.md의 "다른 서버로 배포" 섹션을 참조하세요. 프로젝트 전체를 복사하고 `requirements.txt`로 패키지를 설치하면 됩니다.

### Q: API로 사용할 수 있나요?

A: 현재는 Streamlit 웹 인터페이스만 제공됩니다. API 기능은 향후 추가될 예정입니다.

### Q: 어떤 Ollama 모델을 사용해야 하나요?

A: 문서에 이미지가 포함된 경우 `llava`, 텍스트만 있는 경우 `llama3` 또는 `mistral`을 권장합니다.

### Q: 처리 시간은 얼마나 걸리나요?

A: 파일 크기와 사용하는 처리기에 따라 다릅니다. 일반적으로:
- 작은 파일 (< 1MB): 1-5초
- 중간 파일 (1-10MB): 5-30초
- 큰 파일 (> 10MB): 30초 이상
- Ollama 사용 시 추가 시간 소요

## 추가 리소스

- [Streamlit 문서](https://docs.streamlit.io/)
- [Ollama 문서](https://github.com/ollama/ollama)
- [프로젝트 GitHub](https://github.com/your-repo) (있는 경우)

## 지원

문제가 발생하거나 질문이 있으시면:
1. 이 매뉴얼의 문제 해결 섹션 확인
2. GitHub Issues에 문제 보고
3. 프로젝트 관리자에게 문의


